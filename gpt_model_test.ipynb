{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_with_mask(q, k, v, mask):\n",
    "    # 计算注意力分数\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1)).float())\n",
    "    \n",
    "    # 将mask应用到注意力分数上\n",
    "    attn_scores = attn_scores + mask\n",
    "    \n",
    "    # 使用softmax得到注意力权重\n",
    "    attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "    # 将注意力权重应用到值向量上\n",
    "    attended_values = torch.matmul(attn_weights, v)\n",
    "    \n",
    "    return attended_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[[0.6827, 0.8715, 0.6594, 0.3215, 0.4095, 0.0578, 0.1286, 0.2718],\n",
      "         [0.9719, 0.5628, 0.2867, 0.7069, 0.1371, 0.0905, 0.1335, 0.9883],\n",
      "         [0.7962, 0.2180, 0.1426, 0.8030, 0.1417, 0.1458, 0.3048, 0.5056],\n",
      "         [0.4402, 0.9670, 0.0383, 0.9630, 0.5657, 0.5988, 0.2132, 0.4764],\n",
      "         [0.6218, 0.1403, 0.9986, 0.6016, 0.1755, 0.3002, 0.9370, 0.3618]]])\n"
     ]
    }
   ],
   "source": [
    "# 示例：结合上三角矩阵和Attention机制\n",
    "seq_length = 5\n",
    "tgt_mask = generate_square_subsequent_mask(5)\n",
    "print(tgt_mask)\n",
    "\n",
    "# 假设有一些查询、键和值\n",
    "q = torch.rand((1, seq_length, 8))  # 查询\n",
    "k = torch.rand((1, seq_length, 8))  # 键\n",
    "v = torch.rand((1, seq_length, 16))  # 值\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理解Decoder过程的mask在处理序列交叉中的“不可见”逻辑\n",
    "### 将注意力权重应用到值向量上，attn_weights的行乘以v的列，该操作正是各注意力权重*相应向量再进行加和的结果，因此这样的结果的各个位置token的向量就可以接最后的分类layer(即)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一、 torch.Size([1, 5, 5])\n",
      "tensor([[[0.7243,   -inf,   -inf,   -inf,   -inf],\n",
      "         [0.8397, 0.7308,   -inf,   -inf,   -inf],\n",
      "         [0.6991, 0.5937, 0.3858,   -inf,   -inf],\n",
      "         [0.9776, 0.5494, 0.6603, 0.8436,   -inf],\n",
      "         [0.8244, 0.8459, 0.5046, 1.0104, 0.7843]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5272, 0.4728, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3801, 0.3421, 0.2779, 0.0000, 0.0000],\n",
      "         [0.3073, 0.2003, 0.2237, 0.2687, 0.0000],\n",
      "         [0.2035, 0.2080, 0.1478, 0.2451, 0.1955]]])\n",
      "v torch.Size([1, 5, 16])\n",
      "v tensor([[[0.1223, 0.4286, 0.0514, 0.9821, 0.8721, 0.0588, 0.0726, 0.7067,\n",
      "          0.1589, 0.7693, 0.2967, 0.4090, 0.6279, 0.1535, 0.1420, 0.5737],\n",
      "         [0.2929, 0.9397, 0.2973, 0.6994, 0.3859, 0.4781, 0.5819, 0.1947,\n",
      "          0.6096, 0.4671, 0.1786, 0.4330, 0.6424, 0.8087, 0.0260, 0.9049],\n",
      "         [0.5693, 0.6645, 0.4188, 0.5822, 0.7503, 0.0173, 0.8950, 0.7226,\n",
      "          0.1043, 0.6663, 0.1539, 0.0776, 0.0703, 0.9588, 0.9690, 0.0208],\n",
      "         [0.7383, 0.3679, 0.5192, 0.1659, 0.1308, 0.4294, 0.3173, 0.3805,\n",
      "          0.3041, 0.4704, 0.1943, 0.7642, 0.5955, 0.1110, 0.7463, 0.5736],\n",
      "         [0.2454, 0.9461, 0.6895, 0.5590, 0.7363, 0.8823, 0.3489, 0.5912,\n",
      "          0.8985, 0.1703, 0.5687, 0.8955, 0.8126, 0.5245, 0.2633, 0.4604]]])\n",
      "torch.Size([1, 5, 16])\n",
      "tensor([[[0.1223, 0.4286, 0.0514, 0.9821, 0.8721, 0.0588, 0.0726, 0.7067,\n",
      "          0.1589, 0.7693, 0.2967, 0.4090, 0.6279, 0.1535, 0.1420, 0.5737],\n",
      "         [0.2030, 0.6702, 0.1677, 0.8484, 0.6422, 0.2570, 0.3134, 0.4646,\n",
      "          0.3720, 0.6264, 0.2408, 0.4203, 0.6348, 0.4633, 0.0872, 0.7303],\n",
      "         [0.3049, 0.6690, 0.2376, 0.7743, 0.6719, 0.1907, 0.4753, 0.5359,\n",
      "          0.2979, 0.6373, 0.2166, 0.3251, 0.4779, 0.6014, 0.3321, 0.5334],\n",
      "         [0.4220, 0.5674, 0.3086, 0.6167, 0.5482, 0.2331, 0.4243, 0.5200,\n",
      "          0.2759, 0.6054, 0.2136, 0.4351, 0.4973, 0.4535, 0.4662, 0.5163],\n",
      "         [0.3989, 0.6561, 0.3963, 0.5814, 0.5447, 0.3917, 0.4141, 0.5000,\n",
      "          0.4248, 0.5008, 0.2791, 0.5472, 0.5767, 0.4709, 0.4120, 0.5386]]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(q.size(-1)).float())\n",
    "print(\"一、\",attn_scores.shape)\n",
    "# print(attn_scores)\n",
    "# 将mask应用到注意力分数上\n",
    "attn_scores = attn_scores + tgt_mask\n",
    "# print(\"二、\",attn_scores.shape)\n",
    "print(attn_scores)\n",
    "\n",
    "# 使用softmax得到注意力权重\n",
    "attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)\n",
    "# print(\"三、\",attn_weights.shape)\n",
    "print(attn_weights)\n",
    "\n",
    "\n",
    "print(\"v\",v.shape)\n",
    "print(\"v\",v)\n",
    "# 将注意力权重应用到值向量上，attn_weights的行乘以v的列，该操作正是各注意力权重*相应向量再进行加和的结果，因此这样的结果的各个位置token的向量就可以接最后的分类layer(即)\n",
    "attended_values = torch.matmul(attn_weights, v) #矩阵乘法\n",
    "print(attended_values.shape)\n",
    "print(attended_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[ 5,  6,  9],\n",
      "        [ 7,  8, 10]])\n",
      "tensor([[19, 22, 29],\n",
      "        [43, 50, 67]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "print(A)\n",
    "B = torch.tensor([[5, 6, 9], [7, 8,10]])\n",
    "print(B)\n",
    "result = torch.matmul(A, B)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
